{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn((64,241,101))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:,:,:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z_hat = torch.randn((64,101,40))\n",
    "z_hat = Variable(z_hat,requires_grad=True)\n",
    "z_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = z_hat - a\n",
    "delta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = torch.mean((torch.norm(delta.view(len(a),-1),2,1)**2)) \n",
    "rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta.view(len(a),-1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mel Spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phuc/.local/lib/python3.6/site-packages/numba/errors.py:137: UserWarning: Insufficiently recent colorama version found. Numba requires colorama >= 0.3.9\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import librosa\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchaudio.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(241, 101)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(librosa.stft(x,\n",
    "        hop_length=16000 // 1000 * 10,\n",
    "        n_fft=480)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(241, 101)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randn(241, 101)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 101)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = librosa.feature.melspectrogram(S=x,\n",
    "                                  sr=16000,\n",
    "                                  fmin=20,\n",
    "                                  fmax=4000,\n",
    "                                  n_mels=40)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 241, 101])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y = torch.from_numpy(x.astype(np.float32))\n",
    "# type(y)\n",
    "y = torch.randn((64,241,101))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 40, 101])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melscale_transform = torchaudio.transforms.MelScale(sample_rate=16000, f_min=20, f_max=4000, n_mels =40, n_stft =241, norm =\"slaney\")\n",
    "res = melscale_transform(y)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.7056e-03, -3.3823e-02,  1.0944e-02,  ..., -2.0483e-02,\n",
       "         -9.7354e-03,  8.6827e-03],\n",
       "        [ 2.5997e-02, -1.5191e-02,  8.3311e-03,  ...,  8.2882e-03,\n",
       "         -3.0795e-02, -2.8829e-02],\n",
       "        [ 3.9041e-02,  1.2558e-02,  6.8023e-03,  ..., -6.5897e-04,\n",
       "         -4.7400e-03, -2.0336e-02],\n",
       "        ...,\n",
       "        [-6.8657e-03, -1.9090e-03, -2.6026e-05,  ...,  2.2906e-03,\n",
       "          1.3326e-03, -5.2592e-03],\n",
       "        [ 1.9025e-02,  4.8639e-03,  1.2826e-02,  ...,  1.0902e-02,\n",
       "          8.2801e-04,  8.1999e-03],\n",
       "        [ 1.3835e-02,  3.9362e-04, -5.1417e-03,  ...,  1.2422e-02,\n",
       "          5.1172e-04,  1.3814e-03]], dtype=torch.float64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = torch.from_numpy(data)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0061, -0.0354,  0.0193,  ..., -0.0203, -0.0222,  0.0273],\n",
       "        [ 0.0100, -0.0313,  0.0032,  ..., -0.0173, -0.0018, -0.0065],\n",
       "        [ 0.0157, -0.0245,  0.0139,  ...,  0.0129, -0.0332, -0.0218],\n",
       "        ...,\n",
       "        [-0.0029, -0.0032,  0.0030,  ...,  0.0016, -0.0018,  0.0012],\n",
       "        [ 0.0213,  0.0062,  0.0117,  ...,  0.0136,  0.0031,  0.0078],\n",
       "        [ 0.0124, -0.0008, -0.0067,  ...,  0.0120,  0.0003,  0.0014]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.8492e-03,  1.5935e-03, -8.3467e-03,  ..., -1.3471e-04,\n",
       "          1.2467e-02, -1.8586e-02],\n",
       "        [ 1.6003e-02,  1.6149e-02,  5.0959e-03,  ...,  2.5607e-02,\n",
       "         -2.9026e-02, -2.2295e-02],\n",
       "        [ 2.3329e-02,  3.7022e-02, -7.1457e-03,  ..., -1.3607e-02,\n",
       "          2.8452e-02,  1.4862e-03],\n",
       "        ...,\n",
       "        [-3.9727e-03,  1.3023e-03, -3.0528e-03,  ...,  6.4268e-04,\n",
       "          3.1228e-03, -6.4446e-03],\n",
       "        [-2.2717e-03, -1.3773e-03,  1.1279e-03,  ..., -2.6829e-03,\n",
       "         -2.2622e-03,  4.1282e-04],\n",
       "        [ 1.4684e-03,  1.1577e-03,  1.5513e-03,  ...,  3.7366e-04,\n",
       "          2.2613e-04,  2.7878e-05]], dtype=torch.float64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp - res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.1436e-03, -3.5417e-02, -3.9481e+00,  ..., -2.0349e-02,\n",
       "         -2.2203e-02, -3.6020e+00],\n",
       "        [-4.6058e+00, -3.1340e-02, -5.7337e+00,  ..., -1.7319e-02,\n",
       "         -1.7688e-03, -6.5335e-03],\n",
       "        [-4.1533e+00, -2.4464e-02, -4.2724e+00,  ..., -4.3468e+00,\n",
       "         -3.3192e-02, -2.1822e-02],\n",
       "        ...,\n",
       "        [-2.8930e-03, -3.2112e-03, -5.8003e+00,  ..., -6.4083e+00,\n",
       "         -1.7901e-03, -6.7377e+00],\n",
       "        [-3.8492e+00, -5.0766e+00, -4.4483e+00,  ..., -4.2988e+00,\n",
       "         -5.7795e+00, -4.8553e+00],\n",
       "        [-4.3927e+00, -7.6408e-04, -6.6930e-03,  ..., -4.4189e+00,\n",
       "         -8.1610e+00, -6.6050e+00]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[res > 0] = torch.log(res[res > 0])\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_filters = torch.from_numpy(librosa.filters.dct(40, 40).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct_filters.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-1.2231e+01, -1.3006e+01, -1.5146e+01,  ..., -1.3895e+01,\n",
       "          -1.5004e+01, -1.3099e+01],\n",
       "         [ 1.5328e+00, -4.3706e-01,  1.8887e+00,  ...,  1.6343e+00,\n",
       "           8.0510e+00,  2.8347e+00],\n",
       "         [-3.5855e+00,  1.1071e+00,  1.3882e+00,  ..., -1.7933e+00,\n",
       "           8.8539e-01,  1.4855e+00],\n",
       "         ...,\n",
       "         [ 2.1569e-01,  1.7441e-01, -1.3821e-02,  ..., -1.2061e+00,\n",
       "          -9.4440e-01,  5.1525e-01],\n",
       "         [ 1.1113e+00,  3.1396e+00, -1.1321e+00,  ..., -1.2832e+00,\n",
       "           1.0329e+00,  1.3876e+00],\n",
       "         [-4.2321e-01, -2.5020e-01, -1.0886e+00,  ..., -8.9520e-01,\n",
       "           4.1050e-01, -2.1966e-01]])]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = [torch.matmul(dct_filters, x) for x in torch.split(res, res.shape[1], dim=1)]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([101, 40])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.transpose(res[0], 0, 1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.randn((5,101,40))\n",
    "y_list = [y, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 101, 40])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = torch.cat(y_list)\n",
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(list((torch.unbind(tmp, dim=0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import ChainMap\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import copy\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "import model as mod\n",
    "from manage_audio import AudioPreprocessor\n",
    "from trans import Transformation\n",
    "from dro import DRO\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "my_t = 300\n",
    "\n",
    "\n",
    "class ConfigBuilder(object):\n",
    "    def __init__(self, *default_configs):\n",
    "        self.default_config = ChainMap(*default_configs)\n",
    "\n",
    "    def build_argparse(self):\n",
    "        parser = argparse.ArgumentParser()\n",
    "        for key, value in self.default_config.items():\n",
    "            key = \"--{}\".format(key)\n",
    "            if isinstance(value, tuple):\n",
    "                parser.add_argument(key, default=list(value), nargs=len(value), type=type(value[0]))\n",
    "            elif isinstance(value, list):\n",
    "                parser.add_argument(key, default=value, nargs=\"+\", type=type(value[0]))\n",
    "            elif isinstance(value, bool) and not value:\n",
    "                parser.add_argument(key, action=\"store_true\")\n",
    "            else:\n",
    "                parser.add_argument(key, default=value, type=type(value))\n",
    "        return parser\n",
    "\n",
    "    def config_from_argparse(self, parser=None):\n",
    "        if not parser:\n",
    "            parser = self.build_argparse()\n",
    "        args = vars(parser.parse_known_args()[0])\n",
    "        return ChainMap(args, self.default_config)\n",
    "\n",
    "def print_eval(name, scores, labels, loss, end=\"\\n\"):\n",
    "    batch_size = labels.size(0)\n",
    "    accuracy = (torch.max(scores, 1)[1].view(batch_size).data == labels.data).float().sum() / batch_size\n",
    "    loss = loss.item()\n",
    "    # print(\"{} accuracy: {:>5}, loss: {}\".format(name, accuracy, loss), end=end)\n",
    "    return accuracy.item()\n",
    "\n",
    "def set_seed(config):\n",
    "    seed = config[\"seed\"]\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if not config[\"no_cuda\"]:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "def requires_grad_(model:torch.nn.Module, requires_grad:bool) -> None:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad_(requires_grad)\n",
    "\n",
    "\n",
    "def train(config):\n",
    "    output_dir = os.path.dirname(os.path.abspath(config[\"output_file\"]))\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    model = config[\"model_class\"](config)\n",
    "    if config[\"input_file\"]:\n",
    "        model.load(config[\"input_file\"])\n",
    "    if not config[\"no_cuda\"]:\n",
    "        model.init_weights_glorot()\n",
    "\n",
    "        model.load(config[\"input_file\"])\n",
    "\n",
    "        model = nn.DataParallel(model)\n",
    "        model.to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=config[\"lr\"][0], nesterov=config[\"use_nesterov\"], weight_decay=config[\"weight_decay\"], momentum=config[\"momentum\"])\n",
    "    schedule_steps = config[\"schedule\"]\n",
    "    schedule_steps.append(np.inf)\n",
    "    sched_idx = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # option to use TF from mic\n",
    "    # tf = mod.get_tf_phyaug(my_t)\n",
    "    tf = mod.my_get_tf_phyaug(my_t)\n",
    "\n",
    "    # transformation function\n",
    "    trans = Transformation(tf=tf, sigma=config['sigma'])\n",
    "    trans.to(device)\n",
    "    dro = DRO(writer=writer, sigma=config['sigma'])\n",
    "    \n",
    "    train_set, dev_set, _ = mod.SpeechDataset.splits(config)\n",
    "    train_set.audio_files_to_wav()\n",
    "    dev_set.audio_files_to_wav()\n",
    "\n",
    "    target_loader = None\n",
    "    source_loader = data.DataLoader(\n",
    "        train_set,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=True, drop_last=True,\n",
    "        num_workers = 8,\n",
    "        collate_fn=train_set.collate_fn)\n",
    "    max_train_loader = data.DataLoader(\n",
    "        train_set,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=True, drop_last=True,\n",
    "        num_workers = 8,\n",
    "        collate_fn=None)\n",
    "    dev_loader = data.DataLoader(\n",
    "        dev_set,\n",
    "        batch_size=min(len(dev_set), 64),\n",
    "        shuffle=False,\n",
    "        num_workers = 8,\n",
    "        collate_fn=dev_set.collate_fn)\n",
    "    \n",
    "    # populate main_set\n",
    "    main_set = mod.TensorDataset()\n",
    "\n",
    "    step_no = 0\n",
    "    train_start = time.time()\n",
    "    for epoch_idx in range(config[\"n_epochs\"]):\n",
    "        print(\"Start training .....\")\n",
    "        train_loader = target_loader if target_loader is not None else source_loader\n",
    "        train_bar = tqdm(train_loader, total=len(train_loader))\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_bar):\n",
    "            if epoch_idx == 0:\n",
    "                main_set.add(list(torch.unbind(inputs, dim=0)), list(torch.unbind(labels, dim=0)))\n",
    "\n",
    "            model.train()\n",
    "            requires_grad_(model, True)\n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            inputs = Variable(inputs, requires_grad=False)\n",
    "            labels = Variable(labels, requires_grad=False)\n",
    "\n",
    "            scores = model(inputs)\n",
    "            loss = criterion(scores, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "#             writer.add_scalar(\"Loss/train\", loss.item(), step_no)\n",
    "\n",
    "            batch_size = labels.size(0)\n",
    "            accuracy = (torch.max(scores, 1)[1].view(batch_size).data == labels.data).float().sum() / batch_size\n",
    "            train_bar.set_description(\n",
    "                desc=\"[%d/%d] Loss: %f Acc: %f\"\n",
    "                % (\n",
    "                    epoch_idx,\n",
    "                    config[\"n_epochs\"],\n",
    "                    loss.item(),\n",
    "                    accuracy.item()\n",
    "                )\n",
    "            )\n",
    "\n",
    "            step_no += 1\n",
    "            if step_no > schedule_steps[sched_idx]:\n",
    "                sched_idx += 1\n",
    "                print(\"changing learning rate to {}\".format(config[\"lr\"][sched_idx]))\n",
    "                optimizer = torch.optim.SGD(model.parameters(), lr=config[\"lr\"][sched_idx],\n",
    "                    nesterov=config[\"use_nesterov\"], momentum=config[\"momentum\"], weight_decay=config[\"weight_decay\"])\n",
    "        \n",
    "        # START DRO\n",
    "        if (epoch_idx + 1) % 2 == 0:\n",
    "            print(\"Start DRO .....\")\n",
    "            aug_x, aug_y = [], []\n",
    "            model.eval()\n",
    "            requires_grad_(model, False)\n",
    "            trans.train()\n",
    "            requires_grad_(trans, True)\n",
    "            max_bar = tqdm(max_train_loader, total=len(max_train_loader))\n",
    "            for _, (inputs, labels) in enumerate(max_bar):\n",
    "                for input in dro.forward(inputs, labels, model, trans, _):\n",
    "                    aug_x.append(input.detach().clone().cpu())\n",
    "                    aug_y.append(labels.cpu())\n",
    "            \n",
    "            aug_x = list(torch.unbind(torch.cat(aug_x)))\n",
    "            aug_y = list(torch.unbind(torch.cat(aug_y)))\n",
    "            main_set.add(aug_x, aug_y)\n",
    "            target_loader = data.DataLoader(\n",
    "                main_set,\n",
    "                batch_size=config[\"batch_size\"],\n",
    "                shuffle=True, drop_last=True,\n",
    "                num_workers = 8)\n",
    "        # END DRO\n",
    "    \n",
    "        if epoch_idx % config[\"dev_every\"] == config[\"dev_every\"] - 1:\n",
    "            model.eval()\n",
    "            requires_grad_(model, False)\n",
    "            accs = []\n",
    "            for model_in, labels in dev_loader:\n",
    "                model_in = Variable(model_in, requires_grad=False)\n",
    "                if not config[\"no_cuda\"]:\n",
    "                    model_in = model_in.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                scores = model(model_in)\n",
    "                labels = Variable(labels, requires_grad=False)\n",
    "                loss = criterion(scores, labels)\n",
    "                accs.append(print_eval(\"dev\", scores, labels, loss))\n",
    "            avg_acc = np.mean(accs)\n",
    "            print(\"final dev accuracy: {}\".format(avg_acc))\n",
    "            print(\"saving model...\")\n",
    "            model.module.save(config[\"output_file\"])\n",
    "\n",
    "    train_end = time.time()\n",
    "    print(\"train ended at \",epoch_idx, \"total training time \",(train_end-train_start)/3600,\"hours\")\n",
    "\n",
    "\n",
    "def main(sigma):\n",
    "    model_name = f\"ac2_sigma={sigma}.pt\"\n",
    "    print(\"model name \",model_name)\n",
    "    output_file = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"model\", model_name)\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model\", choices=[x.value for x in list(mod.ConfigType)], default=\"cnn-trad-pool2\", type=str)\n",
    "    config, _ = parser.parse_known_args()\n",
    "\n",
    "    global_config = dict(no_cuda=False, n_epochs=19, lr=[0.001], schedule=[np.inf], batch_size=64, dev_every=10, seed=0,\n",
    "        use_nesterov=False, input_file=\"\", output_file=output_file, \n",
    "        cache_size=32768, \n",
    "        momentum=0.9, weight_decay=0.00001,\n",
    "        sigma = sigma)\n",
    "    mod_cls = mod.find_model(config.model)\n",
    "    builder = ConfigBuilder(\n",
    "        mod.find_config(config.model),\n",
    "        mod.SpeechDataset.default_config(),\n",
    "        global_config)\n",
    "    parser = builder.build_argparse()\n",
    "    \n",
    "    config = builder.config_from_argparse(parser)\n",
    "    config[\"model_class\"] = mod_cls\n",
    "    \n",
    "    config[\"input_file\"] = \"./model/ac1_sigma=0.5.pt\"\n",
    "\n",
    "    set_seed(config)\n",
    "    train(config)\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for sigma in [0.5]:\n",
    "        main(sigma)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
